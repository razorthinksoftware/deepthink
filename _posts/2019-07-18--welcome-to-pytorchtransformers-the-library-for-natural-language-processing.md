---
layout: post
date: "2019-07-18"
author: "Prathyush SP"
link: "https://huggingface.co/pytorch-transformers/index.html"
type: "Tool"
title: "ğŸ¥ğŸ¥ğŸ¥ Welcome to 'pytorch-transformers', the ğŸ‘¾ library for Natural Language Processing!"
tags: ""
comments: true
---
PyTorch-Transformers is a library of state-of-the-art pre-trained models for Natural Language Processing (NLP).

The library currently contains PyTorch implementations, pre-trained model weights, usage scripts and conversion utilities for the following models:

BERTÂ (from Google) released with the paperÂ BERT: Pre-training of Deep Bidirectional Transformers for Language UnderstandingÂ by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.

GPTÂ (from OpenAI) released with the paperÂ Improving Language Understanding by Generative Pre-Trainingby Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.

GPT-2Â (from OpenAI) released with the paperÂ Language Models are Unsupervised Multitask LearnersÂ by Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.

Transformer-XLÂ (from Google/CMU) released with the paperÂ Transformer-XL: Attentive Language Models Beyond a Fixed-Length ContextÂ by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.

XLNetÂ (from Google/CMU) released with the paperÂ â€‹XLNet: Generalized Autoregressive Pretraining for Language UnderstandingÂ by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.

XLMÂ (from Facebook) released together with the paperÂ Cross-lingual Language Model PretrainingÂ by Guillaume Lample and Alexis Conneau.

PyTorch-Transformers is a library of state-of-the-art pre-trained models for Natural Language Processing (NLP).

The library currently contains PyTorch implementations, pre-trained model weights, usage scripts and conversion utilities for the following models:

BERT (from Google) released with the paper BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.

GPT (from OpenAI) released with the paper Improving Language Understanding by Generative Pre-Training by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.

GPT-2 (from OpenAI) released with the paper Language Models are Unsupervised Multitask Learners by Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.

Transformer-XL (from Google/CMU) released with the paper Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.

XLNet (from Google/CMU) released with the paper â€‹XLNet: Generalized Autoregressive Pretraining for Language Understanding by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.

XLM (from Facebook) released together with the paper Cross-lingual Language Model Pretraining by Guillaume Lample and Alexis Conneau.