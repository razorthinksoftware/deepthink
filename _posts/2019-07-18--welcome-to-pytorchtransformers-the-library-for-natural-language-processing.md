---
layout: post
date: "2019-07-18"
author: "Prathyush SP"
link: "https://huggingface.co/pytorch-transformers/index.html"
category: "Tool"
title: "ğŸ¥ğŸ¥ğŸ¥ Welcome to 'pytorch-transformers', the ğŸ‘¾ library for Natural Language Processing!"
tags: ""
comments: true
---
PyTorch-Transformers is a library of state-of-the-art pre-trained models for Natural Language Processing (NLP).

The library currently contains PyTorch implementations, pre-trained model weights, usage scripts and conversion utilities for the following models:

* BERTÂ (from Google) released with the paperÂ BERT
* GPTÂ (from OpenAI) released with the paperÂ Improving Language Understanding 
* GPT-2Â (from OpenAI) released with the paperÂ Language Models are Unsupervised Multitask Learners
* Transformer-XLÂ (from Google/CMU) released with the paperÂ Transformer-XL: Attentive Language Models Beyond a Fixed-Length ContextÂ 
* XLNetÂ (from Google/CMU) released with the paperÂ â€‹XLNet: Generalized Autoregressive Pretraining for Language UnderstandingÂ 
* XLMÂ (from Facebook) released together with the paperÂ Cross-lingual Language Model Pretraining